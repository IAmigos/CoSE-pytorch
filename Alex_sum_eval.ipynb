{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import *\n",
    "from utils import set_seed\n",
    "from data.loaders import *\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "user = 'ajimenez'\n",
    "path_to_model = f\"/home/{user}/CoSE-pytorch/wandb/latest-run/files/weights_trained/epoch_150\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set_seed(0)\n",
    "device = torch.device('cuda:0')\n",
    "cose = CoSEModel('config.json', False)\n",
    "cose.encoder.load_state_dict(torch.load(os.path.join(os.getcwd(), path_to_model,\"encoder.pth\"), map_location=device))\n",
    "cose.decoder.load_state_dict(torch.load(os.path.join(os.getcwd(),path_to_model ,\"decoder.pth\"), map_location=device))\n",
    "cose.position_predictive_model.load_state_dict(torch.load(os.path.join(os.getcwd(),path_to_model,\"pos_pred.pth\"), map_location=device))\n",
    "cose.embedding_predictive_model.load_state_dict(torch.load(os.path.join(os.getcwd(),path_to_model,\"emb_pred.pth\"), map_location=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "cose.encoder = cose.encoder.eval()\n",
    "cose.decoder = cose.decoder.eval()\n",
    "cose.embedding_predictive_model = cose.embedding_predictive_model.eval()\n",
    "cose.position_predictive_model = cose.position_predictive_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_path = f\"/data/{user}/cose/test/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "batchdata = BatchCoSELoader(path = val_path,\n",
    "                    filenames={\"inputs_file\" : \"inputs_list_based.pkl\",\n",
    "                               \"targets_file\": \"target_list_based.pkl\"\n",
    "                              }\n",
    "                )\n",
    "\n",
    "valid_loader = DataLoader(\n",
    "                    dataset =batchdata,\n",
    "                    batch_size = 1, #data is already in batch mode, batch_size = 1 means iterating every .get_next() returns a new batch\n",
    "                )\n",
    "stats_json = 'didi_wo_text-stats-origin_abs_pos.json'\n",
    "stats_path = '/data/jcabrera/didi_wo_text/'\n",
    "with open(os.path.join(stats_path, stats_json)) as json_file:\n",
    "    stats = json.load(json_file)\n",
    "\n",
    "mean_channel = stats['mean_channel'][:2]\n",
    "std_channel = np.sqrt(stats['var_channel'][:2])\n",
    "log_dir = f'/home/ajimenez/pruebas'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "for batch_input, batch_target in iter(valid_loader):\n",
    "    if i == 1:\n",
    "        break\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_loss = AggregateAvg()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_eval_parse_input = eval_parse_input(batch_input, cose.device)\n",
    "out_eval_parse_target = eval_parse_target(batch_target, cose.device)\n",
    "encoder_inputs, _, strok_len_inputs, _, _ = out_eval_parse_input\n",
    "# passing inputs to encoding\n",
    "comb_mask, look_ahead_mask, _ = generate_3d_mask(encoder_inputs, strok_len_inputs,cose.device, cose.config.enc_nhead)\n",
    "encoder_out = cose.encoder(encoder_inputs.permute(1,0,2), strok_len_inputs, comb_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_quant_eval = [cose.embedding_predictive_model, cose.decoder]\n",
    "models_qual_eval = [cose.position_predictive_model, cose.embedding_predictive_model, cose.decoder]\n",
    "stats_tuple = [mean_channel, std_channel]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ajimenez/CoSE-pytorch/models/gmm.py:42: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  out_pi = torch.nn.functional.softmax(out_pi)\n"
     ]
    }
   ],
   "source": [
    "eval_loss, recon_chamfer, pred_chamfer = quantitative_eval_step(encoder_out, out_eval_parse_input, out_eval_parse_target, models_tuple, stats_tuple, eval_loss, cose.device, cose.config.rel_nhead )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cose' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-d1bceec5b766>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mqualitative_eval_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_eval_parse_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_eval_parse_target\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodels_tuple\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstats_tuple\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcose\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcose\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrel_nhead\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/CoSE-pytorch/utils/eval.py\u001b[0m in \u001b[0;36mqualitative_eval_step\u001b[0;34m(encoder_out, out_eval_parse_input, out_eval_parse_target, models, stats_channels, device, rel_nhead)\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mstroke_i\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontext_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_strokes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 136\u001b[0;31m         \u001b[0mcontext_embeddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mar_start_pos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredicted_batch_stroke\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredicted_batch_strat_pos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdraw_seq_len\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdraw_pred_strokes_ar_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstroke_i\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext_embeddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mar_start_pos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmean_channel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstd_channel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrel_nhead\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoded_length\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdraw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    137\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mpredicted_batch_stroke\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredicted_batch_strat_pos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdraw_seq_len\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/CoSE-pytorch/utils/eval.py\u001b[0m in \u001b[0;36mdraw_pred_strokes_ar_step\u001b[0;34m(stroke_i, context_embeddings, ar_start_pos, mean_channel, std_channel, rel_nhead, device, decoded_length, draw)\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[0;31m# mask for position predictive model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m     \u001b[0mseq_mask_rel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseq_len\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcose\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mseq_len\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m     \u001b[0mseq_mask_rel\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mseq_mask_rel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmasked_fill\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseq_mask_rel\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'-inf'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrepeat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mseq_mask_rel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrepeat_interleave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrel_nhead\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'cose' is not defined"
     ]
    }
   ],
   "source": [
    "qualitative_eval_step(encoder_out, out_eval_parse_input, out_eval_parse_target, models_tuple, stats_tuple, cose.device, cose.config.rel_nhead)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'rc_chamfer_stroke': 0.04273923939346498,\n",
       "  'nll_embedding': -2.6050239,\n",
       "  'pred_chamfer_stroke': 0.055498375289230115},\n",
       " 1)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_loss.summary_and_reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
