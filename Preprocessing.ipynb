{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "shuffle = True #called when running TFRecordBatchDiagram on split==C.DATA_TRAIN\n",
    "seed = 1234 #Dataset class\n",
    "num_parallel_calls = 4\n",
    "batch_size = 128\n",
    "\n",
    "## filtering options\n",
    "min_length_threshold = 4 #default value defined in TFRecordStroke that is inherited by TFRecordDiagram\n",
    "max_length_threshold = 201 #defined and default value in config.data\n",
    "num_strokes_threshold = 4 #minimum number of strokes, default value defined in TFRecordStroke that is inherited by TFRecordDiagram\n",
    "\n",
    "#RUN DOES NOT INCLUDE RDP, NOR RDP_DIDI_PP\n",
    "normalize = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tf_data_transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_data_ = tf.data.TFRecordDataset.list_files(['/data/jcabrera/didi_wo_text/training/diagrams_wo_text_20200131-?????-of-?????'], seed = 1234, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorSpec(shape=(), dtype=tf.string, name=None)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_data_.element_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_data_ = tf_data_.interleave(tf.data.TFRecordDataset, cycle_length = num_parallel_calls, block_length = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorSpec(shape=(), dtype=tf.string, name=None)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_data_.element_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "  def parse_tfexample_fn(proto, rdp = True):\n",
    "    \"\"\"Parses a single tfrecord proto storing diagram sequence as strokes.\n",
    "    Args:\n",
    "      proto:\n",
    "    Returns:\n",
    "    \"\"\"\n",
    "\n",
    "    if rdp:\n",
    "      feature_to_type = dict()\n",
    "      feature_to_type[\"rdp_ink\"] = tf.io.VarLenFeature(dtype=tf.float32)\n",
    "      feature_to_type[\"rdp_stroke_length\"] = tf.io.VarLenFeature(dtype=tf.int64)\n",
    "      feature_to_type[\"rdp_num_strokes\"] = tf.io.FixedLenFeature([], dtype=tf.int64)\n",
    "      \n",
    "      parsed_features = tf.io.parse_single_example(serialized=proto, features=feature_to_type)\n",
    "      parsed_features[\"ink\"] = tf.reshape(tf.sparse.to_dense(parsed_features[\"rdp_ink\"]), (parsed_features[\"rdp_num_strokes\"], -1, 4))\n",
    "      parsed_features[\"stroke_length\"] = tf.sparse.to_dense(parsed_features[\"rdp_stroke_length\"])\n",
    "      parsed_features[\"num_strokes\"] = tf.tile(tf.expand_dims(parsed_features[\"rdp_num_strokes\"], axis=0), [parsed_features[\"rdp_num_strokes\"]])\n",
    "    else:\n",
    "      feature_to_type = {\n",
    "          \"ink\": tf.io.VarLenFeature(dtype=tf.float32),\n",
    "          \"stroke_length\": tf.io.VarLenFeature(dtype=tf.int64),\n",
    "          \"num_strokes\": tf.io.FixedLenFeature([], dtype=tf.int64),\n",
    "          # \"shape\": tf.FixedLenFeature([3], dtype=tf.int64),\n",
    "          # \"ink_hash\": tf.FixedLenFeature([], dtype=tf.string),\n",
    "      }\n",
    "      parsed_features = tf.io.parse_single_example(serialized=proto, features=feature_to_type)\n",
    "      parsed_features[\"ink\"] = tf.reshape(tf.sparse.to_dense(parsed_features[\"ink\"]), (parsed_features[\"num_strokes\"], -1, 4))\n",
    "      parsed_features[\"stroke_length\"] = tf.sparse.to_dense(parsed_features[\"stroke_length\"])\n",
    "      parsed_features[\"num_strokes\"] = tf.tile(tf.expand_dims(parsed_features[\"num_strokes\"], axis=0), [parsed_features[\"num_strokes\"]])\n",
    "    \n",
    "    return parsed_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_data_ = tf_data_.map(\n",
    "    functools.partial(parse_tfexample_fn),\n",
    "    num_parallel_calls=num_parallel_calls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rdp_ink': SparseTensorSpec(TensorShape([None]), tf.float32),\n",
       " 'rdp_stroke_length': SparseTensorSpec(TensorShape([None]), tf.int64),\n",
       " 'rdp_num_strokes': TensorSpec(shape=(), dtype=tf.int64, name=None),\n",
       " 'ink': TensorSpec(shape=(None, None, 4), dtype=tf.float32, name=None),\n",
       " 'stroke_length': TensorSpec(shape=(None,), dtype=tf.int64, name=None),\n",
       " 'num_strokes': TensorSpec(shape=(None,), dtype=tf.int64, name=None)}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_data_.element_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_data_ = tf_data_.prefetch(batch_size*2) #boots performance by prestoring 2 batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "  def __pp_filter(sample):\n",
    "    \"\"\"Filters diagram samples.\n",
    "    Works in batch mode. In other words, if an individual stroke of a diagram\n",
    "    violates the conditions, then the entire diagram is discarded.\n",
    "    Hence, the conditions should be relaxed.\n",
    "    Args:\n",
    "      sample:\n",
    "    Returns:\n",
    "    \"\"\"\n",
    "    has_strokes, is_long_enough = True, True\n",
    "    if min_length_threshold > 0:\n",
    "      is_long_enough = tf.math.greater(\n",
    "          tf.reduce_min(input_tensor=sample[\"stroke_length\"]), min_length_threshold)\n",
    "    if max_length_threshold > 0:\n",
    "      is_long_enough = tf.math.logical_and(\n",
    "          is_long_enough,\n",
    "          tf.math.less(\n",
    "              tf.reduce_max(input_tensor=sample[\"stroke_length\"]),\n",
    "              max_length_threshold))\n",
    "    if num_strokes_threshold > 0:\n",
    "      has_strokes = (\n",
    "          tf.shape(input=sample[\"num_strokes\"])[0] > num_strokes_threshold)\n",
    "    return tf.math.logical_and(has_strokes, is_long_enough)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_data_ = tf_data_.filter(functools.partial(__pp_filter)) #filtering for number of strokes, and max length of points in each stroke"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13, 20, 4)\n",
      "(7, 30, 4)\n",
      "(6, 41, 4)\n",
      "(9, 29, 4)\n",
      "(9, 23, 4)\n",
      "(7, 39, 4)\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ajimenez/.local/lib/python3.6/site-packages/IPython/core/interactiveshell.py:3351: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "i = 0\n",
    "for sample_dict in tf_data_:\n",
    "    sample = sample_dict['ink']\n",
    "    if isinstance(sample, tf.Tensor):\n",
    "        sample = sample.numpy()\n",
    "    print (sample.shape)\n",
    "    i+=1\n",
    "    if i>5:\n",
    "        sys.exit(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CONFIG VARIABLES\n",
    "affine_prob = 0.3#default\n",
    "reverse_prob = 0#default\n",
    "scale_factor = 0#default\n",
    "pos_noise_factor = 0#default\n",
    "pp_to_origin = True #in config\n",
    "pp_relative_pos = False #inconfig\n",
    "random_noise_factor = 0#default \n",
    "resampling_factor = 2#default\n",
    "t_drop_ratio = 0#default\n",
    "gt_targets = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from common.constants import Constants as C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_preprocessing(tf_data_):\n",
    "    if affine_prob > 0:\n",
    "        tf_data_ = tf_data_.map(\n",
    "          functools.partial(pp_random_affine_all),\n",
    "          num_parallel_calls=num_parallel_calls)\n",
    "    #Does this\n",
    "    tf_data_ = tf_data_.map(\n",
    "        functools.partial(set_start_end_coord),\n",
    "        num_parallel_calls=num_parallel_calls)\n",
    "    #Does this\n",
    "    if pp_to_origin:\n",
    "        tf_data_ = tf_data_.map(\n",
    "          functools.partial(pp_translate_to_origin),\n",
    "          num_parallel_calls=num_parallel_calls)\n",
    "    if resampling_factor > 1:\n",
    "        tf_data_ = tf_data_.map(\n",
    "          functools.partial(pp_temporal_resampling),\n",
    "          num_parallel_calls=num_parallel_calls)\n",
    "    return tf_data_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "  def pp_random_affine_all(sample):\n",
    "    \"\"\"Applies the same affine transformation to all strokes in a diagram.\"\"\"\n",
    "    \n",
    "    rot_prob = affine_prob\n",
    "    scale_prob = affine_prob\n",
    "    flip_prob = affine_prob\n",
    "    shear_prob = affine_prob/3.0\n",
    "  \n",
    "    n_strokes = tf.shape(input=sample[\"ink\"])[0]\n",
    "    \n",
    "    # Rotation\n",
    "    rot_angle = tf.random.uniform([1],\n",
    "                                  minval=-np.pi/2,\n",
    "                                  maxval=np.pi/2,\n",
    "                                  dtype=tf.float32)\n",
    "    rot_angle = tf.compat.v1.where(rot_prob > tf.random.uniform([1], maxval=1.0),\n",
    "                         rot_angle,\n",
    "                         tf.zeros_like(rot_angle))\n",
    "    rot_angle = tf.tile(rot_angle, [n_strokes])\n",
    "    \n",
    "    # Scale\n",
    "    scale_xy = tf.random.uniform([1],\n",
    "                                 minval=0.5,\n",
    "                                 maxval=2.5,\n",
    "                                 dtype=tf.float32)\n",
    "    scale_xy = tf.compat.v1.where(scale_prob > tf.random.uniform([1], maxval=1.0),\n",
    "                        scale_xy,\n",
    "                        tf.ones_like(scale_xy))\n",
    "    \n",
    "    # Flip around x, y or both.\n",
    "    scale_x = tf.compat.v1.where(flip_prob > tf.random.uniform([1], maxval=1.0),\n",
    "                       scale_xy*-1,\n",
    "                       scale_xy)\n",
    "    scale_y = tf.compat.v1.where(flip_prob > tf.random.uniform([1], maxval=1.0),\n",
    "                       scale_xy*-1,\n",
    "                       scale_xy)\n",
    "\n",
    "    scale_x = tf.tile(scale_x, [n_strokes])\n",
    "    scale_y = tf.tile(scale_y, [n_strokes])\n",
    "    \n",
    "    # Shear\n",
    "    shear_xy = tf.random.uniform([1],\n",
    "                                 minval=-0.3,\n",
    "                                 maxval=0.3,\n",
    "                                 dtype=tf.float32)\n",
    "    shear_xy = tf.compat.v1.where(shear_prob > tf.random.uniform([1], maxval=1.0),\n",
    "                        shear_xy,\n",
    "                        tf.zeros_like(shear_xy))\n",
    "    shear_xy = tf.tile(shear_xy, [n_strokes])\n",
    "    \n",
    "    # Apply affine.\n",
    "    affine_ = apply_affine(sample[\"ink\"][:, :, 0:2],\n",
    "                                theta=rot_angle,\n",
    "                                scale_x=scale_x,\n",
    "                                scale_y=scale_y,\n",
    "                                shear_x=shear_xy,\n",
    "                                shear_y=shear_xy)\n",
    "    augmented = tf.concat([affine_, sample[\"ink\"][:, :, 2:]], axis=-1)\n",
    "    sample[\"ink\"] = augmented\n",
    "    return sample\n",
    "\n",
    "  def pp_temporal_resampling(sample):\n",
    "    \"\"\"Uniform re-sampling over time dimension.\"\"\"\n",
    "    print(\"Temporal resampling factor: \".format(resampling_factor))\n",
    "    if gt_targets:\n",
    "      print(\"Temporal resampling with original targets...\")\n",
    "    else:\n",
    "      print(\"Temporal resampling the targets...\")\n",
    "    \n",
    "    pen = sample[\"ink\"][:, -1:]\n",
    "    factor = tf.cast(\n",
    "        tf.cond(pred=tf.reduce_max(sample[\"stroke_length\"]) < 20,\n",
    "                true_fn=lambda: 1,\n",
    "                false_fn=lambda: resampling_factor // 2),\n",
    "        dtype=tf.int32)\n",
    "\n",
    "    factor = tf.cast(\n",
    "        tf.cond(pred=tf.reduce_max(sample[\"stroke_length\"]) > 100,\n",
    "                true_fn=lambda: resampling_factor,\n",
    "                false_fn=lambda: factor),\n",
    "        dtype=tf.int64)\n",
    "    \n",
    "    if gt_targets:\n",
    "      if \"target_ink\" not in sample:\n",
    "        sample[\"target_ink\"] = sample[\"ink\"]\n",
    "        sample[\"target_stroke_length\"] = sample[\"stroke_length\"]\n",
    "\n",
    "    freq = factor\n",
    "    # freq = tf.random_uniform([1], minval=1, maxval=factor + 1,\n",
    "    #                          dtype=tf.int64)[0]\n",
    "    \n",
    "    sample[\"ink\"] = sample[\"ink\"][:, ::freq, :]\n",
    "    sample[\"ink\"].set_shape((None, None, 4))\n",
    "    # We keep the pen event.\n",
    "    sample[\"ink\"] = tf.concat([sample[\"ink\"][:, :-1], pen], axis=1)\n",
    "    sample[\"stroke_length\"] = tf.cast(\n",
    "        tf.math.ceil(sample[\"stroke_length\"] / freq), tf.int64)\n",
    "    return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "  def apply_affine(sample, theta=0.0, scale_x=1.0, scale_y=1.0,\n",
    "                   shear_x=0.0, shear_y=0.0):\n",
    "    \"\"\"\n",
    "    Affine transformation by applying scaling, rotation and shearing in order.\n",
    "    The sample is a sequence of 2D points. If size of the transformation factors\n",
    "    is equal to batch_size, then the operation runs in batch mode.\n",
    "    The default values correspond to no transformation.\n",
    "    Args:\n",
    "      sample: (batch_size, seq_len, 2)\n",
    "      theta: rotation angle in radians.\n",
    "      scale_x: scale factor in x-axis.\n",
    "      scale_y: scale factor in y-axis.\n",
    "      shear_x: amount of shearing in x direction.\n",
    "      shear_y: amount of shearing in y direction.\n",
    "    Returns:\n",
    "      Transformed sample.\n",
    "    \"\"\"\n",
    "    rot_scale_mat = tf.stack([[scale_x*tf.cos(theta), -scale_y*tf.sin(theta)],\n",
    "                              [scale_x*tf.sin(theta), scale_y*tf.cos(theta)]])\n",
    "    rot_scale_mat = tf.transpose(a=tf.reshape(rot_scale_mat, [2, 2, -1]),\n",
    "                                 perm=[2, 0, 1])\n",
    "    \n",
    "    shear_mat = tf.stack(\n",
    "        [[tf.ones_like(shear_x), shear_x], [shear_y, tf.ones_like(shear_y)]])\n",
    "    shear_mat = tf.transpose(a=tf.reshape(shear_mat, [2, 2, -1]), perm=[2, 0, 1])\n",
    "    \n",
    "    affine_mat = tf.matmul(shear_mat, rot_scale_mat)\n",
    "    return tf.matmul(sample, affine_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "  def set_start_end_coord(sample):\n",
    "    \"\"\"Sets the start and end point coordinates.\"\"\"\n",
    "    sample[C.INP_START_COORD] = sample[\"ink\"][:, 0:1, 0:2]\n",
    "    # Strokes are padded. The end coordinate has the pen-up event where it is 1\n",
    "    # and the rest has pen-up feature 0.\n",
    "    tmp_ = sample[\"ink\"][:, :, 0:2] * sample[\"ink\"][:, :, 3:4]\n",
    "    sample[C.INP_END_COORD] = tf.reduce_sum(input_tensor=tmp_, axis=1, keepdims=True)\n",
    "    return sample\n",
    "\n",
    "  def pp_translate_to_origin(sample):\n",
    "    \"\"\"Translate strokes to origin.\"\"\"\n",
    "    # Batch mode.\n",
    "    if \"target_ink\" in sample:\n",
    "      t_pen_event = sample[\"target_ink\"][:, :, -1:]\n",
    "      t_start_coord = sample[\"target_ink\"][:, 0:1, 0:3]\n",
    "      sample[\"target_ink\"] = tf.concat(\n",
    "          [sample[\"target_ink\"][:, :, 0:3] - t_start_coord, t_pen_event],\n",
    "          axis=-1)\n",
    "\n",
    "    pen_event = sample[\"ink\"][:, :, -1:]\n",
    "    start_coord = sample[\"ink\"][:, 0:1, 0:3]\n",
    "    sample[\"ink\"] = tf.concat(\n",
    "        [sample[\"ink\"][:, :, 0:3] - start_coord, pen_event], axis=-1)\n",
    "\n",
    "    # sample[\"xy_cov\"] = self.sequence_cov(sample[\"ink\"][:, :, 0:2], sample[\"stroke_length\"])\n",
    "    return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temporal resampling factor: \n",
      "Temporal resampling with original targets...\n"
     ]
    }
   ],
   "source": [
    "tf_data_ = tf_preprocessing(tf_data_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rdp_ink': SparseTensorSpec(TensorShape([None]), tf.float32),\n",
       " 'rdp_stroke_length': SparseTensorSpec(TensorShape([None]), tf.int64),\n",
       " 'rdp_num_strokes': TensorSpec(shape=(), dtype=tf.int64, name=None),\n",
       " 'ink': TensorSpec(shape=(None, None, 4), dtype=tf.float32, name=None),\n",
       " 'stroke_length': TensorSpec(shape=(None,), dtype=tf.int64, name=None),\n",
       " 'num_strokes': TensorSpec(shape=(None,), dtype=tf.int64, name=None),\n",
       " 'start_coord': TensorSpec(shape=(None, None, 2), dtype=tf.float32, name=None),\n",
       " 'end_coord': TensorSpec(shape=(None, 1, 2), dtype=tf.float32, name=None),\n",
       " 'target_ink': TensorSpec(shape=(None, None, 4), dtype=tf.float32, name=None),\n",
       " 'target_stroke_length': TensorSpec(shape=(None,), dtype=tf.int64, name=None)}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_data_.element_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9, 38, 4)\n",
      "(9, 29, 4)\n",
      "(7, 30, 4)\n",
      "(12, 38, 4)\n",
      "(5, 24, 4)\n",
      "(7, 26, 4)\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 0\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "i = 0\n",
    "for sample_dict in tf_data_:\n",
    "    sample = sample_dict['ink']\n",
    "    if isinstance(sample, tf.Tensor):\n",
    "        sample = sample.numpy()\n",
    "    print (sample.shape)\n",
    "    i+=1\n",
    "    if i>5:\n",
    "        sys.exit(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rdp_ink': SparseTensorSpec(TensorShape([None]), tf.float32),\n",
       " 'rdp_stroke_length': SparseTensorSpec(TensorShape([None]), tf.int64),\n",
       " 'rdp_num_strokes': TensorSpec(shape=(), dtype=tf.int64, name=None),\n",
       " 'ink': TensorSpec(shape=(None, None, 4), dtype=tf.float32, name=None),\n",
       " 'stroke_length': TensorSpec(shape=(None,), dtype=tf.int64, name=None),\n",
       " 'num_strokes': TensorSpec(shape=(None,), dtype=tf.int64, name=None),\n",
       " 'start_coord': TensorSpec(shape=(None, None, 2), dtype=tf.float32, name=None),\n",
       " 'end_coord': TensorSpec(shape=(None, 1, 2), dtype=tf.float32, name=None),\n",
       " 'target_ink': TensorSpec(shape=(None, None, 4), dtype=tf.float32, name=None),\n",
       " 'target_stroke_length': TensorSpec(shape=(None,), dtype=tf.int64, name=None)}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_data_.element_spec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_data_path = \"/data/jcabrera/didi_wo_text/didi_wo_text-stats-origin_abs_pos.npy\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_meta_data(meta_data_path):\n",
    "    \"\"\"Loads meta-data file given the path.\n",
    "    It is assumed to be in numpy.\n",
    "    Args:\n",
    "        meta_data_path:\n",
    "    Returns:\n",
    "        Meta-data dictionary or False if it is not found.\n",
    "    \"\"\"\n",
    "    # if not meta_data_path or not os.path.exists(meta_data_path):\n",
    "    \n",
    "    _, ext = os.path.splitext(meta_data_path)\n",
    "    if ext == \".json\":\n",
    "      meta_fp = tf.io.gfile.GFile(meta_data_path, \"r\")\n",
    "      try:\n",
    "        meta_fp.size()\n",
    "        print(\"Loading statistics \" + meta_data_path)\n",
    "        json_stats = json.load(meta_fp)\n",
    "        stats_np = dict()\n",
    "        for key_, value_ in json_stats.items():\n",
    "          stats_np[key_] = np.array(value_) if isinstance(value_, list) else \\\n",
    "            value_\n",
    "        return stats_np\n",
    "      except tf.errors.NotFoundError:\n",
    "        print(\"Meta-data not found.\")\n",
    "        return False\n",
    "    \n",
    "    elif ext == \".npy\":\n",
    "      meta_fp = tf.io.gfile.GFile(meta_data_path, \"rb\")\n",
    "      try:\n",
    "        meta_fp.size()\n",
    "        print(\"Loading statistics \" + meta_data_path)\n",
    "        return np.load(meta_fp, allow_pickle=True).item()\n",
    "      except tf.errors.NotFoundError:\n",
    "        print(\"Meta-data not found.\")\n",
    "        return False\n",
    "    else:\n",
    "      err_unknown_type(ext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading statistics /data/jcabrera/didi_wo_text/didi_wo_text-stats-origin_abs_pos.npy\n"
     ]
    }
   ],
   "source": [
    "meta_data = load_meta_data(meta_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_all = meta_data[C.MEAN_ALL]\n",
    "std_all = np.sqrt(meta_data[C.VAR_ALL])\n",
    "mean_channel = meta_data[C.MEAN_CHANNEL]\n",
    "std_channel = np.sqrt(meta_data[C.VAR_CHANNEL])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "  def normalize_zero_mean_unit_variance_channel(sample_dict, key):\n",
    "    sample_dict[key] = (sample_dict[key] - mean_channel)/std_channel\n",
    "    return sample_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "  def pp_seq_mask(sample):\n",
    "    sample[\"ink\"] *= tf.expand_dims(\n",
    "        tf.sequence_mask(sample[\"stroke_length\"], dtype=tf.float32), axis=2)\n",
    "    return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "  def tf_data_normalization(tf_data_):\n",
    "    # Apply normalizatiom inherited from TFRecordStrokes\n",
    "    tf_data_ = tf_data_.map(\n",
    "        functools.partial(\n",
    "            normalize_zero_mean_unit_variance_channel, key=\"ink\"))\n",
    "    if gt_targets and (resampling_factor > 1 or random_noise_factor > 0 or t_drop_ratio > 0):\n",
    "        tf_data_ = tf_data_.map(\n",
    "          functools.partial(\n",
    "              normalize_zero_mean_unit_variance_channel, key=\"target_ink\"))\n",
    "    # After preprocessing and normalization steps, the padded entries\n",
    "    # may have non-zero values. Here we mask them.\n",
    "    tf_data_ = tf_data_.map(\n",
    "        functools.partial(pp_seq_mask),\n",
    "        num_parallel_calls=num_parallel_calls)\n",
    "    return tf_data_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_data_ = tf_data_normalization(tf_data_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6, 41, 4)\n",
      "(7, 34, 4)\n",
      "(13, 20, 4)\n",
      "(7, 30, 4)\n",
      "(7, 39, 4)\n",
      "(8, 31, 4)\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 0\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "i = 0\n",
    "for sample_dict in tf_data_:\n",
    "    sample = sample_dict['ink']\n",
    "    if isinstance(sample, tf.Tensor):\n",
    "        sample = sample.numpy()\n",
    "    print (sample.shape)\n",
    "    i+=1\n",
    "    if i>5:\n",
    "        sys.exit(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rdp_ink': SparseTensorSpec(TensorShape([None]), tf.float32),\n",
       " 'rdp_stroke_length': SparseTensorSpec(TensorShape([None]), tf.int64),\n",
       " 'rdp_num_strokes': TensorSpec(shape=(), dtype=tf.int64, name=None),\n",
       " 'ink': TensorSpec(shape=(None, None, 4), dtype=tf.float32, name=None),\n",
       " 'stroke_length': TensorSpec(shape=(None,), dtype=tf.int64, name=None),\n",
       " 'num_strokes': TensorSpec(shape=(None,), dtype=tf.int64, name=None),\n",
       " 'start_coord': TensorSpec(shape=(None, None, 2), dtype=tf.float32, name=None),\n",
       " 'end_coord': TensorSpec(shape=(None, 1, 2), dtype=tf.float32, name=None),\n",
       " 'target_ink': TensorSpec(shape=(None, None, 4), dtype=tf.float32, name=None),\n",
       " 'target_stroke_length': TensorSpec(shape=(None,), dtype=tf.int64, name=None)}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_data_.element_spec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DATA TO MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'num_strokes'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C.INP_NUM_STROKE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GENERAL CONFIG VARIABLES\n",
    "fixed_len = False\n",
    "concat_t_inputs = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FUNCTION INNER VARIABLES\n",
    "mask_pen = True\n",
    "int_t_samples = False #\n",
    "n_t_targets = 4 #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "  def tf_data_to_model(tf_data):\n",
    "\n",
    "    if fixed_len:\n",
    "        tf_data = tf_data.map(\n",
    "          functools.partial(pp_pad_to_max_len),\n",
    "          num_parallel_calls=num_parallel_calls)\n",
    "\n",
    "    tf_data = tf_data.map(\n",
    "        functools.partial(pp_get_t_targets),\n",
    "        num_parallel_calls=num_parallel_calls)\n",
    "\n",
    "    if concat_t_inputs:\n",
    "        tf_data = tf_data.map(\n",
    "          functools.partial(pp_concat_t_inputs),\n",
    "          num_parallel_calls=num_parallel_calls)\n",
    "    \n",
    "    def element_length_func(model_inputs, _):\n",
    "        return tf.cast(model_inputs[C.INP_NUM_STROKE], tf.int32)\n",
    "\n",
    "    # Converts the data into the format that a model expects.\n",
    "    # Creates input, target, sequence_length, etc.\n",
    "    tf_data = tf_data.map(functools.partial(__to_model_batch))\n",
    "    # TODO configurable bucket_batch_size\n",
    "    if batch_size >= 1:\n",
    "        bucket_batch_size = [\n",
    "          batch_size,\n",
    "          int(math.ceil(batch_size / 2)),\n",
    "          int(math.ceil(batch_size / 3)),\n",
    "          int(math.ceil(batch_size / 4)),\n",
    "          int(math.ceil(batch_size / 5)),\n",
    "      ]\n",
    "        tf_data = tf_data.apply(\n",
    "          tf.data.experimental.bucket_by_sequence_length(\n",
    "              element_length_func=element_length_func,\n",
    "              bucket_batch_sizes=bucket_batch_size,\n",
    "              bucket_boundaries=[8, 13, 18, 23],\n",
    "              pad_to_bucket_boundary=False))\n",
    "    else:\n",
    "        tf_data = tf_data.padded_batch(batch_size=1)\n",
    "    return tf_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "  def __to_model_batch(tf_sample_dict):\n",
    "    \"\"\"Transforms a TFRecord sample into a more general sample representation.\n",
    "    We use global keys to represent the required fields by the models.\n",
    "    Args:\n",
    "        tf_sample_dict:\n",
    "    Returns:\n",
    "    \"\"\"\n",
    "    # Target are the inputs shifted by one step.\n",
    "    # We ignore the timestamp and pen event.\n",
    "    model_input = dict()\n",
    "    ink_ = tf.concat(\n",
    "        [tf_sample_dict[\"ink\"][:, :, 0:2], tf_sample_dict[\"ink\"][:, :, 3:]],\n",
    "        axis=-1)  # Ignore the timestamp.\n",
    "    if mask_pen:\n",
    "      mask_ = tf.sequence_mask(\n",
    "          tf_sample_dict[\"stroke_length\"] - 1,\n",
    "          dtype=tf.float32,\n",
    "          maxlen=tf.reduce_max(input_tensor=tf_sample_dict[\"stroke_length\"]))\n",
    "      model_input[C.INP_SEQ_LEN] = tf_sample_dict[\"stroke_length\"] - 1\n",
    "      model_input[C.INP_ENC] = (ink_ * tf.expand_dims(mask_, axis=2))[:, 0:-1]\n",
    "    else:\n",
    "      model_input[C.INP_SEQ_LEN] = tf_sample_dict[\"stroke_length\"]\n",
    "      model_input[C.INP_ENC] = ink_\n",
    "    model_input[C.INP_DEC] = tf.concat(\n",
    "        [tf.zeros_like(ink_[:, 0:1]), ink_[:, 0:-1]], axis=1)\n",
    "\n",
    "    model_input[C.INP_START_COORD] = tf_sample_dict[C.INP_START_COORD]\n",
    "    model_input[C.INP_END_COORD] = tf_sample_dict[C.INP_END_COORD]\n",
    "    model_input[C.INP_NUM_STROKE] = tf.shape(input=tf_sample_dict[\"stroke_length\"])[0]\n",
    "    model_input[C.INP_T] = tf_sample_dict[C.INP_T]\n",
    "    model_input[C.TARGET_T_INK] = tf_sample_dict[C.TARGET_T_INK]\n",
    "    # model_input[\"xy_cov\"] = tf_sample_dict[\"xy_cov\"]\n",
    "\n",
    "    model_target = dict()\n",
    "    if \"target_ink\" in tf_sample_dict:\n",
    "      ink_t = tf.concat([tf_sample_dict[\"target_ink\"][:, :, 0:2],\n",
    "                         tf_sample_dict[\"target_ink\"][:, :, 3:4]\n",
    "                         ], axis=-1)\n",
    "      model_input[C.INP_DEC] = tf.concat(\n",
    "          [tf.zeros_like(ink_t[:, 0:1]), ink_t[:, 0:-1]], axis=1)\n",
    "  \n",
    "      model_target[\"stroke\"] = tf_sample_dict[\"target_ink\"][:, :, 0:2]\n",
    "      model_target[\"pen\"] = tf_sample_dict[\"target_ink\"][:, :, 3:4]\n",
    "      model_target[C.INP_SEQ_LEN] = tf_sample_dict[\"target_stroke_length\"]\n",
    "      model_target[C.INP_NUM_STROKE] = tf.shape(input=tf_sample_dict[\"target_stroke_length\"])[0]\n",
    "    else:\n",
    "      model_target = dict()\n",
    "      model_target[\"stroke\"] = tf_sample_dict[\"ink\"][:, :, 0:2]\n",
    "      model_target[\"pen\"] = tf_sample_dict[\"ink\"][:, :, 3:4]\n",
    "      model_target[C.INP_SEQ_LEN] = tf_sample_dict[\"stroke_length\"]\n",
    "      model_target[C.INP_NUM_STROKE] = tf.shape(input=tf_sample_dict[\"stroke_length\"])[0]\n",
    "\n",
    "    model_target[C.INP_START_COORD] = model_input[C.INP_START_COORD]\n",
    "    model_target[C.INP_END_COORD] = model_input[C.INP_END_COORD]\n",
    "\n",
    "    model_target[C.TARGET_T_INK] = tf_sample_dict[C.TARGET_T_INK]\n",
    "    model_target[C.TARGET_T_STROKE] = tf_sample_dict[C.TARGET_T_INK][:, :, 0:2]\n",
    "    # timestamp already discarded.\n",
    "    model_target[C.TARGET_T_PEN] = tf_sample_dict[C.TARGET_T_INK][:, :, 2:3]\n",
    "    # model_target[\"xy_cov\"] = tf_sample_dict[\"xy_cov\"]\n",
    "    \n",
    "    return model_input, model_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "  def pp_get_t_targets(sample):\n",
    "    \"\"\"Draw a random t from [0,1] and get the interpolated point in the sequence.\n",
    "    \n",
    "    Handles multiple stroke and multiple t cases.\n",
    "    Args:\n",
    "      sample:\n",
    "    Returns:\n",
    "    \"\"\"\n",
    "    key_len = \"stroke_length\"\n",
    "    key_ink = \"ink\"\n",
    "    if gt_targets and \"target_ink\" in sample:\n",
    "      key_len = \"target_stroke_length\"\n",
    "      key_ink = \"target_ink\"\n",
    "    \n",
    "    if int_t_samples:\n",
    "      n_strokes = tf.shape(input=sample[key_ink])[0]\n",
    "      t = tf.random.uniform([n_strokes, n_t_targets], minval=0, maxval=1,\n",
    "                            dtype=tf.float32)\n",
    "      len_t = t*tf.cast(tf.expand_dims(sample[key_len], axis=-1), tf.float32)\n",
    "      len_t = tf.floor(len_t)\n",
    "      t = len_t / tf.tile(tf.expand_dims(tf.cast(sample[key_len]-1, tf.float32), axis=1), (1, n_t_targets))\n",
    "      lower_idx = tf.cast(len_t, tf.int32)\n",
    "      \n",
    "      batch_indices = tf.ones_like(lower_idx)\n",
    "      batch_indices *= tf.expand_dims(tf.range(n_strokes), axis=-1)\n",
    "      \n",
    "      gather_lower_idx = tf.stack([\n",
    "          batch_indices,\n",
    "          lower_idx\n",
    "          ], axis=-1)\n",
    "\n",
    "      lower_points = tf.gather_nd(sample[key_ink], gather_lower_idx)\n",
    "      inter_points = tf.concat([lower_points[:, :, :-2], lower_points[:, :, -1:]], axis=-1)\n",
    "    else:\n",
    "      n_strokes = tf.shape(input=sample[key_ink])[0]\n",
    "      t = tf.random.uniform([n_strokes, n_t_targets], minval=0, maxval=1,\n",
    "                            dtype=tf.float32)\n",
    "      len_t = t*tf.cast(tf.expand_dims(sample[key_len], axis=-1) - 1,\n",
    "                        tf.float32)\n",
    "      \n",
    "      # Identify lower and upper points.\n",
    "      lower_idx = tf.cast(tf.floor(len_t), tf.int32)\n",
    "      upper_idx = tf.cast(tf.math.ceil(len_t), tf.int32)\n",
    "  \n",
    "      batch_indices = tf.ones_like(lower_idx)\n",
    "      batch_indices *= tf.expand_dims(tf.range(n_strokes), axis=-1)\n",
    "  \n",
    "      gather_lower_idx = tf.stack([\n",
    "          batch_indices,\n",
    "          lower_idx\n",
    "          ], axis=-1)\n",
    "  \n",
    "      gather_upper_idx = tf.stack([\n",
    "          batch_indices,\n",
    "          upper_idx\n",
    "          ], axis=-1)\n",
    "  \n",
    "      lower_points = tf.gather_nd(sample[key_ink], gather_lower_idx)\n",
    "      upper_points = tf.gather_nd(sample[key_ink], gather_upper_idx)\n",
    "  \n",
    "      factor = tf.expand_dims((len_t - tf.floor(len_t)), axis=-1)\n",
    "      inter_points = factor*upper_points + (1 - factor)*lower_points\n",
    "      \n",
    "      max_pen = tf.maximum(lower_points[:, :, -1:], upper_points[:, :, -1:])\n",
    "      inter_points = tf.concat([inter_points[:, :, :-2], max_pen], axis=-1)\n",
    "    sample[C.INP_T] = t\n",
    "    sample[C.TARGET_T_INK] = inter_points\n",
    "    return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_data_ = tf_data_to_model(tf_data_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'seq_len': TensorSpec(shape=(None, None), dtype=tf.int64, name=None),\n",
       "  'encoder_inputs': TensorSpec(shape=(None, None, None, 3), dtype=tf.float32, name=None),\n",
       "  'decoder_inputs': TensorSpec(shape=(None, None, None, 3), dtype=tf.float32, name=None),\n",
       "  'start_coord': TensorSpec(shape=(None, None, None, 2), dtype=tf.float32, name=None),\n",
       "  'end_coord': TensorSpec(shape=(None, None, 1, 2), dtype=tf.float32, name=None),\n",
       "  'num_strokes': TensorSpec(shape=(None,), dtype=tf.int32, name=None),\n",
       "  't_input': TensorSpec(shape=(None, None, 4), dtype=tf.float32, name=None),\n",
       "  't_target_ink': TensorSpec(shape=(None, None, 4, 3), dtype=tf.float32, name=None)},\n",
       " {'stroke': TensorSpec(shape=(None, None, None, 2), dtype=tf.float32, name=None),\n",
       "  'pen': TensorSpec(shape=(None, None, None, 1), dtype=tf.float32, name=None),\n",
       "  'seq_len': TensorSpec(shape=(None, None), dtype=tf.int64, name=None),\n",
       "  'num_strokes': TensorSpec(shape=(None,), dtype=tf.int32, name=None),\n",
       "  'start_coord': TensorSpec(shape=(None, None, None, 2), dtype=tf.float32, name=None),\n",
       "  'end_coord': TensorSpec(shape=(None, None, 1, 2), dtype=tf.float32, name=None),\n",
       "  't_target_ink': TensorSpec(shape=(None, None, 4, 3), dtype=tf.float32, name=None),\n",
       "  't_target_stroke': TensorSpec(shape=(None, None, 4, 2), dtype=tf.float32, name=None),\n",
       "  't_target_pen': TensorSpec(shape=(None, None, 4, 1), dtype=tf.float32, name=None)})"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_data_.element_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "tuple indices must be integers or slices, not str",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-56-d0adb9607703>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msample_dict\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtf_data_\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0msample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msample_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'encoder_inputs'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0msample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: tuple indices must be integers or slices, not str"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "i = 0\n",
    "for sample_dict in tf_data_:\n",
    "    sample = sample_dict['encoder_inputs']\n",
    "    if isinstance(sample, tf.Tensor):\n",
    "        sample = sample.numpy()\n",
    "    print (sample.shape)\n",
    "    i+=1\n",
    "    if i>5:\n",
    "        sys.exit(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<DatasetV1Adapter shapes: ({seq_len: (None, None), encoder_inputs: (None, None, None, 3), decoder_inputs: (None, None, None, 3), start_coord: (None, None, None, 2), end_coord: (None, None, 1, 2), num_strokes: (None,), t_input: (None, None, 4), t_target_ink: (None, None, 4, 3)}, {stroke: (None, None, None, 2), pen: (None, None, None, 1), seq_len: (None, None), num_strokes: (None,), start_coord: (None, None, None, 2), end_coord: (None, None, 1, 2), t_target_ink: (None, None, 4, 3), t_target_stroke: (None, None, 4, 2), t_target_pen: (None, None, 4, 1)}), types: ({seq_len: tf.int64, encoder_inputs: tf.float32, decoder_inputs: tf.float32, start_coord: tf.float32, end_coord: tf.float32, num_strokes: tf.int32, t_input: tf.float32, t_target_ink: tf.float32}, {stroke: tf.float32, pen: tf.float32, seq_len: tf.int64, num_strokes: tf.int32, start_coord: tf.float32, end_coord: tf.float32, t_target_ink: tf.float32, t_target_stroke: tf.float32, t_target_pen: tf.float32})>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_data_.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 12, 89, 3)\n",
      "(128, 7, 63, 3)\n",
      "(64, 12, 86, 3)\n",
      "(43, 17, 80, 3)\n",
      "(64, 12, 61, 3)\n"
     ]
    }
   ],
   "source": [
    "itr = tf_data_.make_one_shot_iterator()\n",
    "for i in range(5):\n",
    "    inputs, targets = itr.get_next()\n",
    "    print(inputs['encoder_inputs'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['seq_len', 'encoder_inputs', 'decoder_inputs', 'start_coord', 'end_coord', 'num_strokes', 't_input', 't_target_ink'])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['stroke', 'pen', 'seq_len', 'num_strokes', 'start_coord', 'end_coord', 't_target_ink', 't_target_stroke', 't_target_pen'])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_data_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "  def tf_data_to_model(self):\n",
    "\n",
    "    if self.fixed_len:\n",
    "      self.tf_data = self.tf_data.map(\n",
    "          functools.partial(self.pp_pad_to_max_len),\n",
    "          num_parallel_calls=self.num_parallel_calls)\n",
    "\n",
    "    self.tf_data = self.tf_data.map(\n",
    "        functools.partial(self.pp_get_t_targets),\n",
    "        num_parallel_calls=self.num_parallel_calls)\n",
    "\n",
    "    if self.concat_t_inputs:\n",
    "      self.tf_data = self.tf_data.map(\n",
    "          functools.partial(self.pp_concat_t_inputs),\n",
    "          num_parallel_calls=self.num_parallel_calls)\n",
    "    \n",
    "    def element_length_func(model_inputs, _):\n",
    "      return tf.cast(model_inputs[C.INP_NUM_STROKE], tf.int32)\n",
    "\n",
    "    # Converts the data into the format that a model expects.\n",
    "    # Creates input, target, sequence_length, etc.\n",
    "    self.tf_data = self.tf_data.map(functools.partial(self.__to_model_batch))\n",
    "    # TODO configurable bucket_batch_size\n",
    "    if self.batch_size >= 1:\n",
    "      bucket_batch_size = [\n",
    "          self.batch_size,\n",
    "          int(math.ceil(self.batch_size / 2)),\n",
    "          int(math.ceil(self.batch_size / 3)),\n",
    "          int(math.ceil(self.batch_size / 4)),\n",
    "          int(math.ceil(self.batch_size / 5)),\n",
    "      ]\n",
    "      self.tf_data = self.tf_data.apply(\n",
    "          tf.data.experimental.bucket_by_sequence_length(\n",
    "              element_length_func=element_length_func,\n",
    "              bucket_batch_sizes=bucket_batch_size,\n",
    "              bucket_boundaries=[8, 13, 18, 23],\n",
    "              pad_to_bucket_boundary=False))\n",
    "    else:\n",
    "      self.tf_data = self.tf_data.padded_batch(batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
