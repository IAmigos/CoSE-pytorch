{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu_shape = (2924,1,2,20)\n",
    "sigma_shape = (2924,1,2,20)\n",
    "pi_shape = (2924,20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pi = torch.rand(pi_shape)\n",
    "mu = torch.randn(mu_shape)\n",
    "sigma = torch.randn(sigma_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pi_indexes = pi.max(dim = 1, keepdim = True)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "component_mu = mu.index_select(dim = 3, index = pi_indexes[-1]).squeeze(dim = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "component_sigma = sigma.index_select(dim = 3, index = pi_indexes[-1]).squeeze(dim = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigma.size(), mu.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_temp(pi_pdf, temp):\n",
    "    # Update to log functinos were made to support 0 values\n",
    "    pi_pdf = torch.log1p(pi_pdf)/temp\n",
    "    pi_pdf -= torch.max(pi_pdf, dim = -1, keepdim=True)[0]\n",
    "    pi_pdf = torch.exp(pi_pdf) - 1.0\n",
    "    pi_pdf /= torch.sum(pi_pdf, dim = -1, keepdim = True)\n",
    "    return pi_pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs = adjust_temp(pi, 0.5)\n",
    "logits = torch.log1p(probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comp_indexes = torch.multinomial(logits, 1).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "component_mu.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.normal(mean = component_mu, std = component_sigma*0.5*0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comp_indexes.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pi_pdf.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pi_pdf.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.sum(pi_pdf, dim = -1, keepdim = True).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pi_pdf /= torch.sum(pi_pdf, dim = -1, keepdim = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pi_pdf = torch.log(pi)/0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pi_pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.max(pi_pdf, dim = -1, keepdim=True)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pi_pdf_v2 = pi_pdf.sub(torch.max(pi_pdf, dim = -1, keepdim=True)[0]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pi_pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adjust_temp(pi, 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pi_pdf = torch.log(pi + 1.0)/.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.max(pi_pdf, dim = -1)[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = torch.tensor([[0, 0, 0, 0],[1, 0,1, 0]]).to(device='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pi_pdf = torch.log(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pi_pdf = torch.log(pi_pdf)/temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pi_pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.max(pi_pdf, dim = -1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.max(pi_pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_sample(outputs: dict, greedy:bool=False, greedy_mu:bool=True, temp:float =0.5):\n",
    "    '''\n",
    "    Obtains 2D strokes from mu, sigma, pis returned by GMM\n",
    "    '''\n",
    "    is_2d = True\n",
    "    if outputs[\"mu\"].dim() == 3:\n",
    "        is_2d = False\n",
    "    out_shape = outputs[\"mu\"].size()\n",
    "    batch_size = out_shape[0]\n",
    "    seq_len = 1 if is_2d else out_shape[1]\n",
    "    comp_shape = (batch_size, seq_len, 2, 20)\n",
    "    #reshapes mu and sigma according to comp_shape\n",
    "    pi = outputs[\"pi\"]\n",
    "    mu = outputs[\"mu\"].reshape(comp_shape)\n",
    "    sigma = outputs[\"sigma\"].reshape(comp_shape)\n",
    "    #permute variables (?, seq_len, out_units, num_components)\n",
    "    mu = mu.permute(0, 1, 3, 2) \n",
    "    sigma = sigma.permute(0, 1, 3, 2)\n",
    "    probs = pi.reshape(-1, 20)\n",
    "    #when greedy: select mus, and sigmas according to max probabilites\n",
    "    if greedy:\n",
    "        logits = torch.log1p(probs) \n",
    "        comp_indexes = logits.max(dim = 1, keepdim = True)[1]\n",
    "    #when not greedy: selects mus, and sigmas according to a categorial distribution with probabilities\n",
    "    else:\n",
    "        probs_adjusted = adjust_temp(pi_pdf, temp)\n",
    "        logits = torch.log1p(probs_adjusted)\n",
    "        comp_indexes = torch.multinominal(logits, 1).reshape(-1, seq_len) #multinomial distribution is categorical\n",
    "    #selects components of mu and sigma according to indexes selected\n",
    "    component_mu = mu.index_select(dim = 3, index = comp_indexes[-1]).squeeze(dim = 3)\n",
    "    component_sigma = sigma.index_select(dim = 3, index = comp_indexes[-1]).squeeze(dim = 3)\n",
    "    #when greedy_mu component mu is the one calculated\n",
    "    if greedy_mu:\n",
    "        sample = component_mu\n",
    "    #when not greedy_mu component mu is equal to a normal distribution with mean=component_mu and std=component_sigma*temp^2\n",
    "    else:\n",
    "        sample = torch.normal(mean = component_mu, std = component_sigma*(temp^2))\n",
    "    if is_2d:\n",
    "        return sample.squeeze() #output shape: (?, 2)\n",
    "    else:\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_features = [512,512,512,512]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[nn.Linear(\n",
    "    in_features=20, \n",
    "    out_features=layer_features[0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn.ModuleList([nn.Linear(\n",
    "    in_features=20, \n",
    "    out_features=layer_features[0])] +\\\n",
    "[nn.Linear(\n",
    "    in_features=layer_features[i], \n",
    "    out_features= layer_features[i+1]) for i in range(len(layer_features)- 1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn.ModuleList(\n",
    "    [nn.Linear(\n",
    "        in_features=size_embedding, \n",
    "        out_features=layer_features[0])] +\n",
    "    [nn.Linear(in_features=layer_features[i+1], out_features= layer_features[i+2]) for i in layer_features]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing for Relational data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating dummy data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_diagrams = 43\n",
    "batch_enc_out = 731\n",
    "embedding_size = 8\n",
    "min_strokes = 12\n",
    "max_strokes = 17"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_out = torch.randn((batch_enc_out, embedding_size)) #encoder output tensor\n",
    "inputs_start_coord = torch.randn(batch_enc_out,1,2) #input start_coord tensor\n",
    "inputs_end_coord =  torch.randn(inputs_start_coord.shape) #input end_coord tensor\n",
    "num_strokes_x_diagram_tensor = torch.randint(min_strokes,max_strokes + 1, size = (num_diagrams,)) #input input_n_strokes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### flujo completo reducido"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\"input_type\": \"hybrid\",\n",
    "                 \"num_predictive_inputs\": 32,\n",
    "                 \"replace_padding\": True,\n",
    "                 \"end_positions\": False,\n",
    "                 \"stop_predictive_grad\": False}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_inputs, pred_input_seq_len, context_pos, pred_targets = random_index_sampling(encoder_out,inputs_start_coord,\n",
    "                                                                                   inputs_end_coord,num_strokes_x_diagram_tensor,\n",
    "                                                                                   input_type =config[\"input_type\"],\n",
    "                                                                                   num_predictive_inputs = config[\"num_predictive_inputs\"],\n",
    "                                                                                   replace_padding = config[\"replace_padding\"],\n",
    "                                                                                   end_positions = config[\"end_positions\"]\n",
    "                                                                                  )\n",
    "pred_targets.requires_grad = False\n",
    "if loaded_config[\"stop_predictive_grad\"]:\n",
    "    pred_inputs.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_diagrams = 43\n",
    "batch_enc_out = 731\n",
    "embedding_size = 8\n",
    "min_strokes = 12\n",
    "max_strokes = 17\n",
    "#\n",
    "encoder_out = torch.randn((batch_enc_out, embedding_size)) #encoder output tensor\n",
    "inputs_start_coord = torch.randn(batch_enc_out,1,2) #input start_coord tensor\n",
    "inputs_end_coord =  torch.randn(inputs_start_coord.shape) #input end_coord tensor\n",
    "num_strokes_x_diagram_tensor = torch.randint(min_strokes,max_strokes + 1, size = (num_diagrams,)) #input input_n_strokes\n",
    "#\n",
    "config = {\"input_type\": \"hybrid\",\n",
    "                 \"num_predictive_inputs\": 32,\n",
    "                 \"replace_padding\": True,\n",
    "                 \"end_positions\": False,\n",
    "                 \"stop_predictive_grad\": False}\n",
    "#\n",
    "pred_inputs, pred_input_seq_len, context_pos, pred_targets = random_index_sampling(encoder_out,inputs_start_coord,\n",
    "                                                                                   inputs_end_coord,num_strokes_x_diagram_tensor,\n",
    "                                                                                   input_type =config[\"input_type\"],\n",
    "                                                                                   num_predictive_inputs = config[\"num_predictive_inputs\"],\n",
    "                                                                                   replace_padding = config[\"replace_padding\"],\n",
    "                                                                                   end_positions = config[\"end_positions\"]\n",
    "                                                                                  )\n",
    "pred_targets.requires_grad = False\n",
    "if config[\"stop_predictive_grad\"]:\n",
    "    pred_inputs.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, embed_size, heads):\n",
    "        super(SelfAttention, self).__init__()\n",
    "        self.embed_size = embed_size\n",
    "        self.heads = heads\n",
    "        self.head_dim = embed_size // heads\n",
    "\n",
    "        assert (\n",
    "            self.head_dim * heads == embed_size\n",
    "        ), \"Embedding size needs to be divisible by heads\"\n",
    "\n",
    "        self.values = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
    "        self.keys = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
    "        self.queries = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
    "        self.fc_out = nn.Linear(heads * self.head_dim, embed_size)\n",
    "\n",
    "    def forward(self, values, keys, query, mask):\n",
    "        # Get number of training examples\n",
    "        N = query.shape[0]\n",
    "\n",
    "        value_len, key_len, query_len = values.shape[1], keys.shape[1], query.shape[1]\n",
    "\n",
    "        # Split the embedding into self.heads different pieces\n",
    "        values = values.reshape(N, value_len, self.heads, self.head_dim)\n",
    "        keys = keys.reshape(N, key_len, self.heads, self.head_dim)\n",
    "        query = query.reshape(N, query_len, self.heads, self.head_dim)\n",
    "\n",
    "        values = self.values(values)  # (N, value_len, heads, head_dim)\n",
    "        keys = self.keys(keys)  # (N, key_len, heads, head_dim)\n",
    "        queries = self.queries(query)  # (N, query_len, heads, heads_dim)\n",
    "\n",
    "        # Einsum does matrix mult. for query*keys for each training example\n",
    "        # with every other training example, don't be confused by einsum\n",
    "        # it's just how I like doing matrix multiplication & bmm\n",
    "\n",
    "        energy = torch.einsum(\"nqhd,nkhd->nhqk\", [queries, keys])\n",
    "        # queries shape: (N, query_len, heads, heads_dim),\n",
    "        # keys shape: (N, key_len, heads, heads_dim)\n",
    "        # energy: (N, heads, query_len, key_len)\n",
    "\n",
    "        # Mask padded indices so their weights become 0\n",
    "        if mask is not None:\n",
    "            energy = energy.masked_fill(mask == 0, float(\"-1e20\"))\n",
    "\n",
    "        # Normalize energy values similarly to seq2seq + attention\n",
    "        # so that they sum to 1. Also divide by scaling factor for\n",
    "        # better stability\n",
    "        attention = torch.softmax(energy / (self.embed_size ** (1 / 2)), dim=3)\n",
    "        # attention shape: (N, heads, query_len, key_len)\n",
    "\n",
    "        out = torch.einsum(\"nhql,nlhd->nqhd\", [attention, values]).reshape(\n",
    "            N, query_len, self.heads * self.head_dim\n",
    "        )\n",
    "        # attention shape: (N, heads, query_len, key_len)\n",
    "        # values shape: (N, value_len, heads, heads_dim)\n",
    "        # out after matrix multiply: (N, query_len, heads, head_dim), then\n",
    "        # we reshape and flatten the last two dimensions.\n",
    "\n",
    "        out = self.fc_out(out)\n",
    "        # Linear layer doesn't modify the shape, final shape will be\n",
    "        # (N, query_len, embed_size)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, embed_size, heads, dropout, forward_expansion):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.attention = SelfAttention(embed_size, heads)\n",
    "        self.norm1 = nn.LayerNorm(embed_size)\n",
    "        self.norm2 = nn.LayerNorm(embed_size)\n",
    "\n",
    "        self.feed_forward = nn.Sequential(\n",
    "            nn.Linear(embed_size, forward_expansion * embed_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(forward_expansion * embed_size, embed_size),\n",
    "        )\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, value, key, query, mask):\n",
    "        attention = self.attention(value, key, query, mask)\n",
    "\n",
    "        # Add skip connection, run through normalization and finally dropout\n",
    "        x = self.dropout(self.norm1(attention + query))\n",
    "        forward = self.feed_forward(x)\n",
    "        out = self.dropout(self.norm2(forward + x))\n",
    "        return out\n",
    "    \n",
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return self.dropout(x)\n",
    "    \n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self,d_model, nhead, dff, nlayers, size_embedding, dropout = 0):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.dense1 = nn.Linear(3, d_model)\n",
    "        self.pos_encoder = PositionalEncoding(d_model, dropout)\n",
    "        self.trans_enc_layers = nn.ModuleList(\n",
    "            [\n",
    "                TransformerBlock(\n",
    "                    d_model,\n",
    "                    nhead,\n",
    "                    dropout=dropout,\n",
    "                    forward_expansion=dff,\n",
    "                )\n",
    "                for _ in range(nlayers)\n",
    "            ]\n",
    "        )        \n",
    "        self.dense2 = nn.Linear(d_model, size_embedding)\n",
    "        self.dropout = nn.Dropout(dropout) #Attention is all you need page 7. In addition, we apply dropout to the sums of the embeddings and the\n",
    "                                           #positional encodings in both the encoder and decoder stacks.\n",
    "    \n",
    "    def get_last_time_step(self, tensor, stroke_lengths):\n",
    "        \n",
    "        embeddingd_lt = []\n",
    "        \n",
    "        for pos_embedding in range(tensor.shape[0]):\n",
    "            embedding = tensor[pos_embedding, stroke_lengths[pos_embedding]-1,:]\n",
    "            embeddingd_lt.append(embedding)\n",
    "        \n",
    "        embeddingd_lt = torch.vstack(embeddingd_lt) \n",
    "        \n",
    "        return embeddingd_lt\n",
    "    \n",
    "    def forward(self, src, stroke_lengths, src_mask):\n",
    "        \n",
    "        output = self.dense1(src)\n",
    "        output = self.pos_encoder(output)\n",
    "        output = self.dropout(output)\n",
    "        \n",
    "        for layer in self.trans_enc_layers:\n",
    "            output = layer(output, output, output, src_mask)\n",
    "\n",
    "        output = self.get_last_time_step(output, stroke_lengths)\n",
    "        \n",
    "        output = self.dense2(output)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing data for embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "config_ = json.load(open(\"config.json\", 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "from models import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "cose_model  = CoSEModel(**config_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = pickle.load(open(os.path.join(\"D:\\Projects\\RC-PWC2020\\data\", \"inputs_list_based.pkl\"), 'rb'))\n",
    "targets = pickle.load(open(os.path.join(\"D:\\Projects\\RC-PWC2020\\data\", \"target_list_based.pkl\"), 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_input = inputs[0]\n",
    "sample_target = targets[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_, target_ = batch_diagram_to_stroke(sample_input, sample_target, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_inputs = torch.from_numpy(input_['encoder_inputs'])\n",
    "t_inputs = torch.from_numpy(input_['t_input'])\n",
    "stroke_len_inputs = torch.from_numpy(input_['seq_len'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_square_subsequent_mask(sz):\n",
    "    mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n",
    "    mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_3d_mask(seq_input, seq_len_input):\n",
    "    look_ahead_mask = generate_square_subsequent_mask(seq_input.shape[1])\n",
    "    seq_mask = 1 - (torch.arange(stroke_len_inputs.max().item())[None, :] < stroke_len_inputs[:, None]).float()\n",
    "    seq_mask  = seq_mask.masked_fill(seq_mask == 1, float('-inf')).unsqueeze(dim=2).repeat(1,1,seq_mask.shape[1])\n",
    "    return torch.minimum(seq_mask, look_ahead_mask), look_ahead_mask, seq_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_3d_mask, look_ahead_mask, seq_mask = generate_3d_mask(enc_inputs, stroke_len_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([97, 768, 64])\n",
      "tensor([[[ 0.0530,  1.5284, -1.4227,  ..., -0.9789, -1.0885, -0.8036],\n",
      "         [ 0.1772,  1.5447, -1.3344,  ..., -0.8030, -1.0910, -0.7979],\n",
      "         [ 0.3444,  1.3610, -1.4185,  ..., -1.0445, -0.8237, -1.3872],\n",
      "         ...,\n",
      "         [-0.1718,  0.4918, -1.3834,  ..., -0.7728,  1.6515,  0.4093],\n",
      "         [-0.0378,  0.4513, -1.4079,  ..., -0.7473,  1.6977,  0.2563],\n",
      "         [-0.1174,  0.1912, -1.6401,  ..., -0.7678,  1.7473,  0.1135]],\n",
      "\n",
      "        [[ 0.0530,  1.5284, -1.4227,  ..., -0.9789, -1.0885, -0.8036],\n",
      "         [ 0.2134,  0.8767, -1.7480,  ..., -1.3055, -0.2065, -1.6600],\n",
      "         [ 0.2260,  0.1671, -2.0990,  ..., -1.2285,  0.4287, -1.7383],\n",
      "         ...,\n",
      "         [-0.3428, -1.6862, -1.7424,  ..., -1.2098,  2.3888,  1.1468],\n",
      "         [-0.2578, -1.6595, -1.8023,  ..., -1.1795,  2.4979,  1.0440],\n",
      "         [-0.2620, -1.8088, -2.0301,  ..., -1.1767,  2.4686,  0.8790]],\n",
      "\n",
      "        [[ 0.0530,  1.5284, -1.4227,  ..., -0.9789, -1.0885, -0.8036],\n",
      "         [ 0.2634,  1.0889, -1.7224,  ..., -1.1543, -0.8642, -1.4616],\n",
      "         [ 0.2245,  0.8336, -1.8706,  ..., -0.7642, -0.5266, -0.4249],\n",
      "         ...,\n",
      "         [-0.3326,  0.7177, -1.5975,  ..., -1.0422,  1.4580,  0.0843],\n",
      "         [-0.2146,  0.6668, -1.6243,  ..., -1.0145,  1.5186, -0.0989],\n",
      "         [-0.2961,  0.3912, -1.8958,  ..., -1.0107,  1.5743, -0.2594]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 0.0530,  1.5284, -1.4227,  ..., -0.9789, -1.0885, -0.8036],\n",
      "         [ 0.1942,  1.5609, -1.3336,  ..., -0.8165, -1.0846, -0.7965],\n",
      "         [ 0.2584,  1.4914, -1.3186,  ..., -0.7448, -0.9708, -0.8779],\n",
      "         ...,\n",
      "         [-0.2656,  0.5842, -1.4600,  ..., -0.8537,  1.5737,  0.1905],\n",
      "         [-0.1462,  0.5375, -1.4921,  ..., -0.8251,  1.6356,  0.0153],\n",
      "         [-0.2274,  0.2564, -1.7555,  ..., -0.8514,  1.6949, -0.1568]],\n",
      "\n",
      "        [[ 0.0530,  1.5284, -1.4227,  ..., -0.9789, -1.0885, -0.8036],\n",
      "         [ 0.1804,  1.5456, -1.3344,  ..., -0.8061, -1.0911, -0.8014],\n",
      "         [ 0.2178,  1.4604, -1.3191,  ..., -0.7101, -0.9896, -0.8738],\n",
      "         ...,\n",
      "         [-0.2739,  0.6749, -1.4066,  ..., -0.8314,  1.4963,  0.1010],\n",
      "         [-0.1560,  0.6364, -1.4333,  ..., -0.8066,  1.5647, -0.0678],\n",
      "         [-0.2429,  0.3523, -1.6997,  ..., -0.8240,  1.6300, -0.2326]],\n",
      "\n",
      "        [[ 0.1368,  1.2792, -1.8077,  ..., -1.4800, -0.7923, -0.6445],\n",
      "         [ 0.3376,  1.3133, -1.7315,  ..., -1.2874, -0.7858, -0.6414],\n",
      "         [ 0.4150,  1.2275, -1.7205,  ..., -1.1850, -0.5992, -0.7128],\n",
      "         ...,\n",
      "         [-0.3260,  0.7346, -1.5391,  ..., -1.0515,  1.3547,  0.0746],\n",
      "         [-0.2111,  0.6869, -1.5690,  ..., -1.0267,  1.4195, -0.1008],\n",
      "         [-0.2891,  0.4138, -1.8338,  ..., -1.0268,  1.4809, -0.2588]]],\n",
      "       grad_fn=<PermuteBackward>)\n"
     ]
    }
   ],
   "source": [
    "out_encoder = cose_model.encoder(enc_inputs.permute(1,0,2), stroke_len_inputs, look_ahead_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.5962,  1.6801,  0.5153,  ..., -1.4957, -0.4100,  0.0826],\n",
       "        [ 0.2414,  0.7255,  0.1064,  ..., -0.6304,  0.0044,  0.5023],\n",
       "        [ 0.6303,  0.8308,  0.5827,  ..., -0.8371, -0.2658, -0.2817],\n",
       "        ...,\n",
       "        [ 0.6275,  1.5211,  0.6182,  ..., -1.3546, -0.2110,  0.0343],\n",
       "        [ 0.4640,  1.3644,  0.5649,  ..., -1.3547, -0.2803,  0.0281],\n",
       "        [ 0.4412,  1.4317,  0.7230,  ..., -1.4038, -0.6683,  0.3492]],\n",
       "       grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_pred = torch.randn(1,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "gmm = torch.randn(1,9,258)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 9, 260])"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cat([gmm, pos_pred.unsqueeze(dim = 1).repeat(1,9,1)], dim = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_components = 10\n",
    "out_size = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs ={\"mu\": torch.randn(1, num_components*out_size), \"sigma\": torch.randn(1,num_components*out_size),\n",
    "             \"pi\": torch.randn(1,num_components)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 10, 2])\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1, 1, 2])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 2])"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "draw_sample(outputs, greedy = True).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_sample(outputs: dict, greedy:bool=False, greedy_mu:bool=True, temp:float =0.5):\n",
    "    '''\n",
    "    Obtains 2D strokes from mu, sigma, pis returned by GMM\n",
    "    '''\n",
    "    is_2d = True\n",
    "    if outputs[\"mu\"].dim() == 3:\n",
    "        is_2d = False\n",
    "    out_shape = outputs[\"mu\"].size()\n",
    "    batch_size = out_shape[0]\n",
    "    seq_len = 1 if is_2d else out_shape[1]\n",
    "    comp_shape = (batch_size, seq_len, out_size, num_components)\n",
    "    #reshapes mu and sigma according to comp_shape\n",
    "    pi = outputs[\"pi\"]\n",
    "    mu = outputs[\"mu\"].reshape(comp_shape)\n",
    "    sigma = outputs[\"sigma\"].reshape(comp_shape)\n",
    "    #permute variables (?, seq_len, out_units, num_components)\n",
    "    mu = mu.permute(0, 1, 3, 2)\n",
    "    print(mu.shape)\n",
    "    sigma = sigma.permute(0, 1, 3, 2)\n",
    "    probs = pi.reshape(-1, num_components)\n",
    "    #when greedy: select mus, and sigmas according to max probabilites\n",
    "    if greedy:\n",
    "        logits = torch.log1p(probs) \n",
    "        comp_indexes = logits.max(dim = 1, keepdim = True)[1]\n",
    "    #when not greedy: selects mus, and sigmas according to a categorial distribution with probabilities\n",
    "    else:\n",
    "        probs_adjusted = adjust_temp(pi_pdf, temp)\n",
    "        logits = torch.log1p(probs_adjusted)\n",
    "        comp_indexes = torch.multinominal(logits, 1).reshape(-1, seq_len) #multinomial distribution is categorical\n",
    "    #selects components of mu and sigma according to indexes selected\n",
    "    print(comp_indexes.shape)\n",
    "    component_mu = mu.index_select(dim = 2, index = comp_indexes[-1]).squeeze(dim = 2)\n",
    "    print(component_mu.shape)\n",
    "    component_sigma = sigma.index_select(dim = 2, index = comp_indexes[-1]).squeeze(dim = 2)\n",
    "    #when greedy_mu component mu is the one calculated\n",
    "    if greedy_mu:\n",
    "        sample = component_mu\n",
    "    #when not greedy_mu component mu is equal to a normal distribution with mean=component_mu and std=component_sigma*temp^2\n",
    "    else:\n",
    "        sample = torch.normal(mean = component_mu, std = component_sigma*(temp^2))\n",
    "    if is_2d:\n",
    "        return sample.squeeze(dim = 1) #output shape: (?, 2)\n",
    "    else:\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([74496, 97, 97])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'seq_mask' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-d045d89535a0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mseq_mask\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'seq_mask' is not defined"
     ]
    }
   ],
   "source": [
    "seq_mask.shape, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (768) must match the size of tensor b (97) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-97-98ae0e3bffe9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mminimum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mseq_mask\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlook_ahead_mask\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m: The size of tensor a (768) must match the size of tensor b (97) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "torch.minimum(seq_mask, look_ahead_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_mask = 1 - (torch.arange(stroke_len_inputs.max().item())[None, :] < stroke_len_inputs[:, None]).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([97, 768, 64])\n",
      "tensor([[[ 0.1954, -0.4692,  0.0050,  ...,  0.6069, -0.7033,  0.0290],\n",
      "         [ 0.3912, -0.4092,  0.2105,  ...,  0.6060, -0.8481, -0.0680],\n",
      "         [ 0.3291, -1.1856,  0.0725,  ...,  0.2461, -0.2257,  0.4046],\n",
      "         ...,\n",
      "         [-0.0497, -0.2687,  0.4462,  ...,  0.1759, -0.4275, -0.0027],\n",
      "         [ 0.1757, -0.2384,  0.5353,  ...,  0.1459, -0.4374, -0.0524],\n",
      "         [ 0.2562, -0.4149,  0.4614,  ...,  0.0879, -0.3387, -0.0135]],\n",
      "\n",
      "        [[ 0.1954, -0.4692,  0.0050,  ...,  0.6069, -0.7033,  0.0290],\n",
      "         [-0.1080, -0.7571,  0.7001,  ..., -0.4141, -0.4302,  0.0192],\n",
      "         [-0.1558, -0.6740,  0.9561,  ..., -0.5954, -0.5528,  0.2184],\n",
      "         ...,\n",
      "         [ 0.1108,  0.2183,  1.2628,  ..., -0.4207, -1.1018,  0.5429],\n",
      "         [ 0.2621,  0.2270,  1.2946,  ..., -0.4309, -1.1176,  0.4959],\n",
      "         [ 0.3240,  0.1114,  1.2112,  ..., -0.4660, -1.0362,  0.5310]],\n",
      "\n",
      "        [[ 0.1954, -0.4692,  0.0050,  ...,  0.6069, -0.7033,  0.0290],\n",
      "         [ 0.3439, -0.6228,  0.6906,  ...,  0.0664, -0.4765,  0.1833],\n",
      "         [ 0.4973, -0.1918,  1.0180,  ...,  0.1441, -1.1647, -0.0923],\n",
      "         ...,\n",
      "         [-0.1252, -0.2437,  0.8107,  ...,  0.5146, -0.1835, -0.0992],\n",
      "         [ 0.1542, -0.2179,  0.9376,  ...,  0.4742, -0.2265, -0.1853],\n",
      "         [ 0.2545, -0.4581,  0.8703,  ...,  0.3855, -0.0962, -0.1377]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 0.1954, -0.4692,  0.0050,  ...,  0.6069, -0.7033,  0.0290],\n",
      "         [ 0.3858, -0.4651,  0.2165,  ...,  0.5816, -0.8090, -0.0429],\n",
      "         [ 0.4023, -0.4918,  0.3270,  ...,  0.4913, -0.8701, -0.1114],\n",
      "         ...,\n",
      "         [-0.0956, -0.2707,  0.6000,  ...,  0.3795, -0.3035, -0.0618],\n",
      "         [ 0.1645, -0.2455,  0.7068,  ...,  0.3271, -0.3162, -0.1252],\n",
      "         [ 0.2532, -0.4528,  0.6462,  ...,  0.2488, -0.1996, -0.0828]],\n",
      "\n",
      "        [[ 0.1954, -0.4692,  0.0050,  ...,  0.6069, -0.7033,  0.0290],\n",
      "         [ 0.3945, -0.4174,  0.2170,  ...,  0.6016, -0.8393, -0.0671],\n",
      "         [ 0.4262, -0.3603,  0.3326,  ...,  0.5383, -0.9612, -0.1976],\n",
      "         ...,\n",
      "         [-0.1350, -0.3161,  0.5522,  ...,  0.4551, -0.2561, -0.0807],\n",
      "         [ 0.1283, -0.2915,  0.6686,  ...,  0.4160, -0.2834, -0.1468],\n",
      "         [ 0.2176, -0.5039,  0.6109,  ...,  0.3379, -0.1666, -0.1067]],\n",
      "\n",
      "        [[ 0.0616, -0.5173,  0.7250,  ...,  0.1185, -0.2713,  0.2134],\n",
      "         [ 0.2616, -0.4681,  0.9631,  ...,  0.1214, -0.4454,  0.1192],\n",
      "         [ 0.2731, -0.4843,  1.0309,  ...,  0.0033, -0.6210,  0.0765],\n",
      "         ...,\n",
      "         [-0.1333, -0.3075,  0.7476,  ...,  0.4537, -0.1789, -0.2144],\n",
      "         [ 0.1363, -0.2838,  0.8718,  ...,  0.4118, -0.2224, -0.2914],\n",
      "         [ 0.2309, -0.5112,  0.8104,  ...,  0.3282, -0.0973, -0.2436]]],\n",
      "       grad_fn=<PermuteBackward>)\n"
     ]
    }
   ],
   "source": [
    "out_encoder = cose_model.encoder(enc_inputs.permute(1,0,2), stroke_len_inputs, look_ahead_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.1809, -0.1351,  0.2037,  ..., -0.7732, -0.4463,  0.2469],\n",
       "        [ 0.6715, -0.1332,  0.5573,  ..., -0.8245, -0.3798,  0.1617],\n",
       "        [ 0.7992,  0.0052,  0.7707,  ..., -0.6442, -1.2051, -0.1041],\n",
       "        ...,\n",
       "        [ 1.0841, -0.0080,  0.1744,  ..., -0.7723, -0.6316,  0.2348],\n",
       "        [ 1.3098, -0.0625,  0.2096,  ..., -0.8533, -0.6813,  0.1241],\n",
       "        [ 1.0203, -0.2480,  0.2407,  ..., -0.7391, -0.5876,  0.5197]],\n",
       "       grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.sum(out_encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "  def batch_diagram_to_stroke(inputs, targets, model_inp_size):\n",
    "    \"\"\"Converts batch of diagrams into batch of strokes.\n",
    "    Reshapes [num_diagrams, num_strokes, ...] shaped inputs and targets into\n",
    "    [num_diagrams x num_strokes, ...]. Note that num_strokes are padded to the\n",
    "    maximum number of strokes in the batch.\n",
    "    Args:\n",
    "      inputs (dict): model inputs, i.e., return of __to_model_batch.\n",
    "      targets (dict): model targets, i.e., return of __to_model_batch.\n",
    "    Returns:\n",
    "      (tuple) of reshaped inputs and targets.\n",
    "    \"\"\"\n",
    "    n_samples = inputs['encoder_inputs'].shape[0]\n",
    "    n_strokes = inputs['encoder_inputs'].shape[1]\n",
    "    batch_dim =  n_samples*n_strokes\n",
    "    \n",
    "    inputs['encoder_inputs'] = inputs['encoder_inputs'].reshape([batch_dim, -1, model_inp_size])\n",
    "    inputs['decoder_inputs'] = inputs['decoder_inputs'].reshape([batch_dim, -1, model_inp_size])\n",
    "    inputs['seq_len'] = inputs['seq_len'].reshape([batch_dim])\n",
    "    inputs['start_coord'] = inputs['start_coord'].reshape([batch_dim, 1, 2])\n",
    "    inputs['end_coord'] = inputs['end_coord'].reshape([batch_dim, 1, 2])\n",
    "    \n",
    "    targets[\"stroke\"] = targets[\"stroke\"].reshape([batch_dim, -1, 2])\n",
    "    targets[\"pen\"] = targets[\"pen\"].reshape([batch_dim, -1, 1])\n",
    "    targets['seq_len'] = targets['seq_len'].reshape([batch_dim])\n",
    "    targets['start_coord'] = targets['start_coord'].reshape([batch_dim, 1, 2])\n",
    "    targets['end_coord'] = targets['end_coord'].reshape([batch_dim, 1, 2])\n",
    "\n",
    "    inputs['t_input'] = inputs['t_input'].reshape([batch_dim, -1])\n",
    "    inputs['t_target_ink'] = inputs['t_target_ink'].reshape([-1, 3])\n",
    "    targets['t_target_ink'] = targets['t_target_ink'].reshape([-1, 3])\n",
    "    targets['t_target_pen'] = targets['t_target_pen'].reshape([-1, 1])\n",
    "    targets['t_target_stroke'] = targets['t_target_stroke'].reshape([-1, 2])\n",
    "    targets[\"stroke_mask\"] = torch.where(torch.from_numpy(inputs['t_input']) > 0., torch.ones_like(torch.from_numpy(inputs['t_input'])),torch.zeros_like(torch.from_numpy(inputs['t_input']))).reshape([-1])\n",
    "\n",
    "    return inputs, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#config variables \n",
    "input_type = \"hybrid\"\n",
    "num_predictive_inputs = 32 #DEPENDS HEAVILY ON DATA\n",
    "replace_padding = True\n",
    "end_positions = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diagram_embedding, padded_max_num_strokes, min_n_stroke, num_diagrams = reshape_stroke2diagram(encoder_out,num_strokes_x_diagram_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_n_inputs = []\n",
    "all_input_indexes = []\n",
    "all_target_indexes = []\n",
    "all_seq_len = []\n",
    "\n",
    "if input_type == \"hybrid\":\n",
    "    num_predictive_inputs //= 2\n",
    "\n",
    "if input_type in [\"random\", \"hybrid\"]:\n",
    "    for i in range(num_predictive_inputs):\n",
    "        input_indexes, target_indexes, n_inputs = get_random_inp_target_pairs(num_strokes_x_diagram_tensor,\n",
    "                                                                              padded_max_num_strokes,\n",
    "                                                                              num_diagrams,\n",
    "                                                                              min_n_stroke)\n",
    "        all_input_indexes.append(input_indexes)        \n",
    "        all_target_indexes.append(target_indexes)\n",
    "        all_n_inputs.append(n_inputs)\n",
    "        all_seq_len.append(torch.ones([num_diagrams])*n_inputs)\n",
    "    \n",
    "if input_type in [\"order\", \"hybrid\"]:\n",
    "    for i in range(num_predictive_inputs):\n",
    "        input_indexes, target_indexes, n_inputs = get_ordered_inp_target_pairs(num_strokes_x_diagram_tensor,\n",
    "                                                                               padded_max_num_strokes,\n",
    "                                                                               num_diagrams,\n",
    "                                                                               min_n_stroke)\n",
    "        all_input_indexes.append(input_indexes)\n",
    "        all_target_indexes.append(target_indexes)\n",
    "        all_n_inputs.append(n_inputs)\n",
    "        all_seq_len.append(torch.ones([num_diagrams])*n_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#preparing for tensor indexing\n",
    "input_range_n_inputs = torch.arange(start=0, end = num_diagrams).repeat(1,len(all_n_inputs)).permute(1,0).squeeze()\n",
    "gather_target_index = torch.stack([input_range_n_inputs,\n",
    "             torch.cat(all_target_indexes, dim = 0).squeeze()], dim = -1)\n",
    "start_pos_base = inputs_start_coord.reshape(num_diagrams,padded_max_num_strokes,2)\n",
    "end_pos_base = inputs_end_corrd.reshape(num_diagrams,padded_max_num_strokes,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gathering indexes from base tensors\n",
    "pred_targets = torch.stack([diagram_embedding[i][j] for i,j in gather_target_index])\n",
    "pred_inputs = gather_indexes(diagram_embedding, all_input_indexes, replace_padding = True)\n",
    "pred_input_seq_len = torch.cat(all_seq_len,dim=0)\n",
    "if end_positions:\n",
    "    start_pos = gather_indexes(start_pos_base, all_input_indexes, replace_padding = True)\n",
    "    end_pos = gather_indexes(end_pos_base, all_input_indexes, replace_padding = True)\n",
    "    context_pos = torch.cat([start_pos, end_pos], dim=-1)\n",
    "else:\n",
    "    context_pos = gather_indexes(start_pos_base, all_input_indexes, replace_padding = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_index_sampling(encoder_out,inputs_start_coord,inputs_end_coord,num_strokes_x_diagram_tensor,\n",
    "                         input_type =\"hybrid\", num_predictive_inputs = 32, replace_padding = True, end_positions = False):\n",
    "    \n",
    "    #obtains diagram embedding (batch_strokes, embedding_size) -> (num_diagrams, padded_n_strokes, embedding_size)\n",
    "    diagram_embedding, padded_max_num_strokes, min_n_stroke, num_diagrams = reshape_stroke2diagram(encoder_out,num_strokes_x_diagram_tensor)\n",
    "    #creates indexes to gather from\n",
    "    all_n_inputs = []\n",
    "    all_input_indexes = []\n",
    "    all_target_indexes = []\n",
    "    all_seq_len = []\n",
    "\n",
    "    if input_type == \"hybrid\":\n",
    "        num_predictive_inputs //= 2\n",
    "\n",
    "    if input_type in [\"random\", \"hybrid\"]:\n",
    "        for i in range(num_predictive_inputs):\n",
    "            input_indexes, target_indexes, n_inputs = get_random_inp_target_pairs(num_strokes_x_diagram_tensor,\n",
    "                                                                                  padded_max_num_strokes,\n",
    "                                                                                  num_diagrams,\n",
    "                                                                                  min_n_stroke)\n",
    "            all_input_indexes.append(input_indexes)        \n",
    "            all_target_indexes.append(target_indexes)\n",
    "            all_n_inputs.append(n_inputs)\n",
    "            all_seq_len.append(torch.ones([num_diagrams])*n_inputs)\n",
    "\n",
    "    if input_type in [\"order\", \"hybrid\"]:\n",
    "        for i in range(num_predictive_inputs):\n",
    "            input_indexes, target_indexes, n_inputs = get_ordered_inp_target_pairs(num_strokes_x_diagram_tensor,\n",
    "                                                                                   padded_max_num_strokes,\n",
    "                                                                                   num_diagrams,\n",
    "                                                                                   min_n_stroke)\n",
    "            all_input_indexes.append(input_indexes)\n",
    "            all_target_indexes.append(target_indexes)\n",
    "            all_n_inputs.append(n_inputs)\n",
    "            all_seq_len.append(torch.ones([num_diagrams])*n_inputs)\n",
    "\n",
    "    #preparing for tensor indexing\n",
    "    input_range_n_inputs = torch.arange(start=0, end = num_diagrams).repeat(1,len(all_n_inputs)).permute(1,0).squeeze()\n",
    "    gather_target_index = torch.stack([input_range_n_inputs,\n",
    "                 torch.cat(all_target_indexes, dim = 0).squeeze()], dim = -1)\n",
    "    start_pos_base = inputs_start_coord.reshape(num_diagrams,padded_max_num_strokes,2)\n",
    "    end_pos_base = inputs_end_coord.reshape(num_diagrams,padded_max_num_strokes,2)\n",
    "    #gathering indexes from base tensors\n",
    "    pred_targets = torch.stack([diagram_embedding[i][j] for i,j in gather_target_index])\n",
    "    pred_inputs = gather_indexes(diagram_embedding, all_input_indexes, replace_padding = True)\n",
    "    pred_input_seq_len = torch.cat(all_seq_len,dim=0)\n",
    "    if end_positions:\n",
    "        start_pos = gather_indexes(start_pos_base, all_input_indexes, replace_padding = True)\n",
    "        end_pos = gather_indexes(end_pos_base, all_input_indexes, replace_padding = True)\n",
    "        context_pos = torch.cat([start_pos, end_pos], dim=-1)\n",
    "    else:\n",
    "        context_pos = gather_indexes(start_pos_base, all_input_indexes, replace_padding = True)\n",
    "    return pred_inputs, pred_input_seq_len, context_pos, pred_targets\n",
    "\n",
    "def gather_indexes(base_tensor, index_tensor, replace_padding = True):\n",
    "    '''\n",
    "    Simulates gather_nd and has an extra property to replace padding with values in first (stroke) in first (diagram)\n",
    "    Args:\n",
    "        index_tensor: list of var-len tensors\n",
    "        base_tensor: tensor from which to sample from\n",
    "    Returns:\n",
    "        gathered_tensor_padded: tensor sampled and replaced in padding if option replace_padding = True\n",
    "    '''\n",
    "    tensor_var_len = [base_tensor[i,value,:].squeeze() for a in index_tensor for i,value in enumerate(a)]\n",
    "    gathered_tensor_padded = torch.nn.utils.rnn.pad_sequence(tensor_var_len, batch_first=False, padding_value=0).permute(1,0,2)\n",
    "    if replace_padding:\n",
    "        gathered_tensor_padded = replacing_padding_with_embedding(base_tensor, gathered_tensor_padded, index_diagram = 0, index_stroke = 0)    \n",
    "    return gathered_tensor_padded\n",
    "\n",
    "def replacing_padding_with_embedding(ref_tensor, tensor_to_mod, index_diagram= 0, index_stroke= 0):\n",
    "    '''\n",
    "    replaces padded values with values from dia\n",
    "    '''\n",
    "    default_first_base_tensor = ref_tensor[0,0,:].repeat(tensor_to_mod.shape[0],tensor_to_mod.shape[1],1)\n",
    "    return torch.where(tensor_to_mod != 0.0, tensor_to_mod, default_first_base_tensor)\n",
    "\n",
    "def reshape_stroke2diagram(stroke_embedding,num_strokes_x_diagram_tensor):\n",
    "    padded_max_n_strokes = torch.max(num_strokes_x_diagram_tensor).item()\n",
    "    min_n_stroke = torch.min(num_strokes_x_diagram_tensor).item()\n",
    "    num_diagrams = num_strokes_x_diagram_tensor.shape[0]\n",
    "    diagram_embedding = stroke_embedding.reshape([num_diagrams, padded_max_n_strokes, embedding_size])\n",
    "    return diagram_embedding, padded_max_n_strokes, min_n_stroke, num_diagrams\n",
    "\n",
    "def get_random_inp_target_pairs(num_strokes_x_diagram_tensor, padded_max_num_strokes, num_diagrams, min_n_stroke):\n",
    "    \"\"\"Get a randomly generated input set and a target.\"\"\"\n",
    "    n_inputs = torch.randint(2, (min_n_stroke+1),size = (1,)).item()\n",
    "    target_indexes = (torch.rand([num_diagrams])*num_strokes_x_diagram_tensor).int().reshape(-1,1)\n",
    "    input_range = torch.arange(start=1, end = padded_max_num_strokes + 1).repeat(num_diagrams,1)\n",
    "    mask = (input_range <= num_strokes_x_diagram_tensor.reshape(-1,1)) & (input_range != target_indexes)\n",
    "    input_indexes = torch.multinomial((input_range*mask).float(),n_inputs) - 1\n",
    "    return input_indexes, target_indexes, n_inputs\n",
    "\n",
    "def get_ordered_inp_target_pairs(num_strokes_x_diagram_tensor, padded_max_num_strokes, num_diagrams, min_n_stroke, random_target = False):\n",
    "    \"\"\"Get a slice (i.e., window) randomly.\"\"\"\n",
    "    n_inputs = torch.randint(2, (min_n_stroke+1),size = (1,)).item()\n",
    "    start_index = torch.randint(0, min_n_stroke - n_inputs + 1, size = (1,)).item()\n",
    "    if not random_target:\n",
    "        target_indexes = torch.tensor([start_index+n_inputs]).repeat(num_diagrams,1)\n",
    "    else:\n",
    "        target_indexes = (torch.rand([num_diagrams])*num_strokes_x_diagram_tensor).int().reshape(-1,1)\n",
    "    input_range = torch.arange(start=1, end = padded_max_num_strokes + 1).repeat(num_diagrams,1)\n",
    "    mask = ((input_range - 1)< target_indexes) & ((input_range - 1)>= start_index)\n",
    "    input_indexes = input_range.masked_select(mask).reshape(num_diagrams, n_inputs)\n",
    "    return input_indexes, target_indexes, n_inputs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
